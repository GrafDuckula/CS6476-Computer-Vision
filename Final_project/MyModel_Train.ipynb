{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23055,
     "status": "ok",
     "timestamp": 1575392005495,
     "user": {
      "displayName": "Graf. D",
      "photoUrl": "",
      "userId": "03653251726372635600"
     },
     "user_tz": 300
    },
    "id": "3CpK4NsRrAwM",
    "outputId": "bab05fe4-673c-4a24-8eb1-c683a83aaf4d"
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = '/content/drive/My Drive/Final_project/data'\n",
    "    model_dir = '/content/drive/My Drive/Final_project/model'\n",
    "    output_dir = '/content/drive/My Drive/Final_project/output'\n",
    "else:\n",
    "    data_dir = './data'\n",
    "    model_dir = './model'\n",
    "    output_dir = './output'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1575392007743,
     "user": {
      "displayName": "Graf. D",
      "photoUrl": "",
      "userId": "03653251726372635600"
     },
     "user_tz": 300
    },
    "id": "p7GsopMjrAwY",
    "outputId": "5527cce6-ab41-4fd5-c817-d922a2add443"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from scipy.io import loadmat\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from sklearn.svm import LinearSVC\n",
    "# from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "use_GPU = torch.cuda.is_available()\n",
    "if use_GPU:\n",
    "    print(\"Using CUDA\")\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    print (\"Using CPU\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14358,
     "status": "ok",
     "timestamp": 1575392024903,
     "user": {
      "displayName": "Graf. D",
      "photoUrl": "",
      "userId": "03653251726372635600"
     },
     "user_tz": 300
    },
    "id": "IxJcn9llrAwc",
    "outputId": "e100dd9a-0129-412a-8198-c562e85b0e4e"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize([48, 48]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])]) # orignally for ImageNet\n",
    "\n",
    "\n",
    "svhn = {\n",
    "    x: datasets.SVHN(root=data_dir, split= x, download=True, transform=transform)\n",
    "    for x in ['train', 'test']\n",
    "}\n",
    "\n",
    "# use test dataset of cifar10 as negative control, and labeled as 10\n",
    "cifar = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "cifar.targets[:] = [10]*len(cifar)\n",
    "\n",
    "print (len(cifar))\n",
    "print (len(svhn['train']))\n",
    "print (len(svhn['test']))\n",
    "\n",
    "comb_datasets = torch.utils.data.ConcatDataset([svhn['train'], cifar])\n",
    "\n",
    "val_ratio = .01\n",
    "random_seed= 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "train_size = len(comb_datasets)\n",
    "indices = list(range(train_size))\n",
    "\n",
    "split = int(np.floor(val_ratio * train_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(\n",
    "        comb_datasets, batch_size=128, shuffle=False, num_workers=8, sampler=train_sampler\n",
    "    ),\n",
    "    'val': torch.utils.data.DataLoader(\n",
    "        comb_datasets, batch_size=128, shuffle=False, num_workers=8, sampler=val_sampler\n",
    "    ),\n",
    "    'test': torch.utils.data.DataLoader(\n",
    "        svhn['test'], batch_size=128, shuffle=True, num_workers=8\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "dataset_sizes = {'train': len(train_indices),\n",
    "                 'val': len(val_indices),\n",
    "                 'test': len(svhn['test'])}\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjJrbtrOrAwf"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_pct = 1.0\n",
    "\n",
    "def train(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    train_start = time.time()\n",
    "    num_train_batches = len(dataloaders['train'])   \n",
    "    num_val_batches = len(dataloaders['val'])\n",
    "    num_test_batches = len(dataloaders['test'])\n",
    "    \n",
    "    best_acc = 0\n",
    "    \n",
    "    ave_loss_train = 0\n",
    "    ave_loss_val = 0\n",
    "    ave_loss_test = 0\n",
    "    ave_acc_train = 0\n",
    "    ave_acc_val = 0\n",
    "    ave_acc_test = 0\n",
    "    \n",
    "    count = 0 # number of validation accuracy decreases\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print (\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "        print ('--' * 5)\n",
    "        \n",
    "        loss_train_sum = 0\n",
    "        loss_val_sum = 0\n",
    "        loss_test_sum = 0\n",
    "        acc_train_sum = 0\n",
    "        acc_val_sum = 0\n",
    "        acc_test_sum = 0\n",
    "        \n",
    "        \n",
    "        for i, data in enumerate(dataloaders['train']):\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rTraining batch {}/{}\".format(i, num_train_batches*train_pct), end='', flush=True)\n",
    "                \n",
    "            if i >= num_train_batches*train_pct:\n",
    "                break\n",
    "            \n",
    "            images, labels = data\n",
    "            \n",
    "            images.requires_grad_(True)\n",
    "            # labels.requires_grad_(True)\n",
    "            \n",
    "            if use_GPU:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train_sum += loss.item()\n",
    "            acc_train_sum += torch.sum(torch.eq(preds, labels.data))\n",
    "            \n",
    "            del images, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # # save check point\n",
    "        # model_name = 'VGG16'\n",
    "        # date = 'Nov_25'\n",
    "        # # status = 'pretrained'\n",
    "        # status = 'random'\n",
    "        # PATH = F'/content/drive/My Drive/Final_project/model/{model_name}_{status}_{date}_epoch_{epoch}.pt'\n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'model_state_dict': model.state_dict(),\n",
    "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #     'loss': loss,\n",
    "        #     }, PATH)\n",
    "        \n",
    "        model.train(False)\n",
    "        model.eval()\n",
    "        \n",
    "        print ()\n",
    "        print ('Validating')\n",
    "\n",
    "        for i, data in enumerate(dataloaders['val']):\n",
    "            \n",
    "            images, labels = data\n",
    "            \n",
    "            images.requires_grad_(True)\n",
    "            # labels.requires_grad_(True)\n",
    "            \n",
    "            if use_GPU:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_val_sum += loss.item()\n",
    "            acc_val_sum += torch.sum(torch.eq(preds, labels.data))\n",
    "            \n",
    "            del images, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()   \n",
    "        \n",
    "        \n",
    "        \n",
    "        print ()\n",
    "        print ('testing')\n",
    "\n",
    "        for i, data in enumerate(dataloaders['test']):\n",
    "            \n",
    "            images, labels = data\n",
    "            \n",
    "            images.requires_grad_(True)\n",
    "            # labels.requires_grad_(True)\n",
    "            \n",
    "            if use_GPU:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_test_sum += loss.item()\n",
    "            acc_test_sum += torch.sum(torch.eq(preds, labels.data))\n",
    "            \n",
    "            del images, labels, outputs, preds\n",
    "            torch.cuda.empty_cache() \n",
    "        \n",
    "        avg_loss_train = loss_train_sum * 1.0/ dataset_sizes['train'] / train_pct\n",
    "        avg_acc_train = acc_train_sum * 1.0/ dataset_sizes['train'] / train_pct\n",
    "        avg_loss_val = loss_val_sum * 1.0 / dataset_sizes['val']\n",
    "        avg_acc_val = acc_val_sum * 1.0/ dataset_sizes['val']\n",
    "        avg_loss_test = loss_test_sum * 1.0/ dataset_sizes['test']\n",
    "        avg_acc_test = acc_test_sum * 1.0 / dataset_sizes['test']\n",
    "        \n",
    "        epoch_length = time.time() - epoch_start\n",
    "        \n",
    "        print ()\n",
    "        print ()\n",
    "        print (\"Epoch {} result: \".format(epoch+1))\n",
    "        print (\"Avg loss of training: {:.4f}\".format(avg_loss_train))\n",
    "        print (\"Avg acc of training: {:.4f}\".format(avg_acc_train))\n",
    "        print (\"Avg loss of validation: {:.4f}\".format(avg_loss_val))\n",
    "        print (\"Avg acc of validation: {:.4f}\".format(avg_acc_val))\n",
    "        print (\"Avg loss of test: {:.4f}\".format(avg_loss_test))\n",
    "        print (\"Avg acc of test: {:.4f}\".format(avg_acc_test))\n",
    "        print ()\n",
    "        print (\"Epoch completed in {:.0f}m {:.0f}s\".format(epoch_length // 60, epoch_length % 60))\n",
    "        print ('--' * 5)\n",
    "        print ()\n",
    "        \n",
    "        if avg_acc_val > best_acc:\n",
    "            best_acc = avg_acc_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            count = 0\n",
    "        else:\n",
    "            count +=1\n",
    "        print (\"count = \", count)\n",
    "        print (\"avg_acc_val\", avg_acc_val)\n",
    "        print (\"best_acc\", best_acc)\n",
    "\n",
    "        # earlier stop\n",
    "        if count >= 6:\n",
    "            print ('earlier stop')\n",
    "            break \n",
    "        \n",
    "    training_time = time.time() - train_start\n",
    "    print ()\n",
    "    print (\"Training completed in {:.0f}m {:.0f}s\".format(training_time // 60, training_time % 60))\n",
    "    print (\"Best acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8dhoPElrAwi"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "drop_rate = 0.2\n",
    "\n",
    "class my_model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(my_model, self).__init__()\n",
    "\n",
    "        self._hidden1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=48, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=0),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=160, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=160),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=0),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=160, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=0),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=0),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self._hidden9 = nn.Sequential(\n",
    "            nn.Linear(192 * 3 * 3, 3072),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self._hidden10 = nn.Sequential(\n",
    "            nn.Linear(3072, 3072),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self._output = nn.Sequential(nn.Linear(3072, 11))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._hidden1(x)\n",
    "        x = self._hidden2(x)\n",
    "        x = self._hidden3(x)\n",
    "        x = self._hidden4(x)\n",
    "        x = self._hidden5(x)\n",
    "        x = self._hidden6(x)\n",
    "        x = self._hidden7(x)\n",
    "        x = self._hidden8(x)\n",
    "        x = x.view(x.size(0), 192 * 3 * 3)\n",
    "        x = self._hidden9(x)\n",
    "        x = self._hidden10(x)\n",
    "\n",
    "        output = self._output(x)  \n",
    "        \n",
    "        return output\n",
    "\n",
    "# test = my_model()\n",
    "# print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1567686,
     "status": "ok",
     "timestamp": 1575393684597,
     "user": {
      "displayName": "Graf. D",
      "photoUrl": "",
      "userId": "03653251726372635600"
     },
     "user_tz": 300
    },
    "id": "vy2f2NCZrAwm",
    "outputId": "efd718ef-174c-4b6b-ff14-4ca4d7f4696e"
   },
   "outputs": [],
   "source": [
    "# run my model\n",
    "\n",
    "my_model = my_model()\n",
    "\n",
    "model_save_name = 'MyModel_SVHN_Dec_03.pt'\n",
    "\n",
    "if colab = True:\n",
    "    model_path = F'/content/drive/My Drive/Final_project/model/{model_save_name}' \n",
    "else:\n",
    "    model_path = F'./model/{model_save_name}'\n",
    "    \n",
    "\n",
    "resume_training = False\n",
    "\n",
    "if resume_training:\n",
    "    my_model.load_state_dict(torch.load(model_path))\n",
    "    print('My model loaded!')\n",
    "    \n",
    "if use_GPU:\n",
    "    my_model.cuda()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(my_model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "my_model = train(my_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=50)\n",
    "torch.save(my_model.state_dict(), model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MyModel_Train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
